{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing Results \n",
    "\n",
    "Most of the experiments involved the DomainNet and Visda2017 datasets, which require us to fine-tune Resnet50-101 architectures. Each run takes a while (up to 8 hours) and requires a GPU accelerator. Therefore in this notebook I demonstrate how to rerun the experiments with different hyperparameters on a slurm cluster, like Izar on Scitas. \n",
    "\n",
    "The launch.py script contains a class called experimentBuilder, with methods for setting hyperparameters, such as setAlpha. When initializing an experiment builder, you pass it a list of paths to .yaml files that contain all the necessary configuration (see experiment_registry) for the experiments you wish to run. These base experiment configuration files contain details such as the architecture, source and target domain datasets, and so on. Calling a builder method like \"setAlpha\" will produce versions of all the base_experiments with all the variations of the parameters you pass the builder method. \n",
    "\n",
    "For example, calling .setAlpha([1,2,3]).setEta([2,3]) will make a total of 6 experiments with all the combinations of the Alpha and Eta parameters. \n",
    "\n",
    "After calling these builder methods, you can launch all experiments by calling .launch() on the experiment builder class. When this is done, it will create a dict of the config and pass it to a batch submission script for each of the generated experiments, which in turn passes it as an input to the run_experiment.py script, which will execute the experiment for that configuratio on one of the cluster nodes. \n",
    "\n",
    "All the metrics are logged on weights and biases, and can be accessed with the weights and biases API for plotting and analysis. Below I give a brief demo of how the slurm submission script works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#let's first change the working directory to /robust-domain-adaptation\n",
    "#if you're not running this notebook from there, run the command below.\n",
    "%cd .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "base_path = \"/home/mielonen/robust-domain-adaptation\"\n",
    "\n",
    "#select the experiments on which all the runs are based, can be any file on the experiment registry directory.\n",
    "base_experiments = [\n",
    "    \"experiment_registry/mcc-domain.yaml\"\n",
    "]\n",
    "\n",
    "#append the base path\n",
    "base_experiments = [os.path.join(base_path, e) for e in base_experiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "def yamlToDict(full_path):\n",
    "    with open(full_path, \"r\") as stream:\n",
    "        dict_ = yaml.safe_load(stream)\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#here's a copy of the experimentBuilder class from launch.py. \n",
    "#the idea is to create a set of nested generators using the builder design pattern. \n",
    "# .launch() will then create an os command for each of the dictionaries it receives from the final generator. \n",
    "\n",
    "class experimentBuilder:\n",
    "\n",
    "    \"\"\"\n",
    "    Usage: \n",
    "    experimentBuilder(base_experiments).setSeed([1,2,3]).setCfol(True).launch() \n",
    "\n",
    "    Explanation:\n",
    "    For each base experiment set seeds to [1, 2, 3], Cfol to true, and submit everything as a slurm batch job. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fnames):\n",
    "\n",
    "        self.base_list = [yamlToDict(name) for name in fnames]\n",
    "\n",
    "        def base():\n",
    "            for dic in self.base_list: \n",
    "                yield dic\n",
    "\n",
    "        self.generator = base()\n",
    "\n",
    "    def setSeed(self, list_):\n",
    "\n",
    "        def new(base):\n",
    "            for dict_ in base:\n",
    "                for seed in list_:\n",
    "                    newDict = dict_\n",
    "                    newDict[\"experiment\"][\"global_params\"][\"seed\"] = seed\n",
    "                    yield newDict\n",
    "\n",
    "        self.generator = new(self.generator)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def setGamma(self, list_):\n",
    "\n",
    "        def new(base):\n",
    "            for dict_ in base:\n",
    "                for gamma in list_:\n",
    "                    newDict = dict_\n",
    "                    assert(gamma > 0)\n",
    "                    assert(gamma < 1)\n",
    "                    newDict[\"experiment\"][\"global_params\"][\"cfol_gamma\"] = gamma\n",
    "                    yield newDict\n",
    "\n",
    "        self.generator = new(self.generator)\n",
    "        return self\n",
    "\n",
    "    def setMethod(self, val):\n",
    "    \n",
    "        def new(base):\n",
    "            for dict_ in base:\n",
    "                newDict = dict_\n",
    "                newDict[\"experiment\"][\"global_params\"][\"reweight_method\"] = val\n",
    "                yield newDict\n",
    "\n",
    "        self.generator = new(self.generator)\n",
    "        return self\n",
    "\n",
    "    def setEta(self, list_):\n",
    "\n",
    "        def new(base):\n",
    "            for dict_ in base:\n",
    "                for eta in list_:\n",
    "                    newDict = dict_\n",
    "                    newDict[\"experiment\"][\"global_params\"][\"cfol_eta\"] = eta\n",
    "                    yield newDict\n",
    "\n",
    "        self.generator = new(self.generator)\n",
    "        return self\n",
    "\n",
    "    def setAlpha(self, list_):\n",
    "\n",
    "        def new(base):\n",
    "            for dict_ in base:\n",
    "                for alpha in list_:\n",
    "                    newDict = dict_\n",
    "                    newDict[\"experiment\"][\"global_params\"][\"cvar_alpha\"] = alpha\n",
    "                    yield newDict\n",
    "\n",
    "        self.generator = new(self.generator)\n",
    "        return self\n",
    "\n",
    "    def launch(self):\n",
    "        a = self.generator\n",
    "        for i in a:\n",
    "            jsonString = json.dumps(i)\n",
    "            str_cmd = f\"sbatch submit_job.run '{jsonString}'\"\n",
    "            os.system(str_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#this sets the seed to 12, the reweighting method to lcvar, and lcvar alpha to 0.80. \"launch\" calls sbatch and submits a job with a json version of the generated config dictionary.\n",
    "experimentBuilder(base_experiments).setSeed([12]).setMethod(\"lcvar\").setAlpha([0.80]).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#if you want to go crazy with hyperparameter tuning, you can also make the following call: \n",
    "experimentBuilder(base_experiments).setSeed([12,13,14]).setMethod(\"lcvar\").setAlpha([0.5,0.7,0.9]).launch()\n",
    "experimentBuilder(base_experiments).setSeed([12,13,14]).setMethod(\"cfol\").setEta([0.01,0.001,0.000001]).launch()\n",
    "#if base_experiments were to contain all 6 base versions of the experiments (2 different datasets for each of the 3 methods), \n",
    "#these two lines would submit a total of 108 slurm jobs, for each of the possible variations of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reproduce all results, you would set base_experiments to a list of all files in the experiment_registry. Then, you would make one call for setMethod = lcvar, one for setMethod = cfol, giving a total of 12 experiments. To gather statistics you would select at least 3 different seeds, for a total of 36, and so on. If you have any questions, feel free to reach out to me at: \n",
    "\n",
    "eelis.mielonen@epfl.ch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test4",
   "language": "python",
   "name": "test4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
